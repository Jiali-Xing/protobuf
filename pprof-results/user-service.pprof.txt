Total: 1.06s
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).LoadShedding in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/charon.go
         0       10ms (flat, cum)  0.94% of Total
         .          .     96:func (pt *PriceTable) LoadShedding(ctx context.Context, tokens int64, methodName string) (int64, error) {
         .          .     97:	// if pt.loadShedding is false, then return tokens and nil error
         .          .     98:	if !pt.loadShedding {
         .          .     99:		return tokens, nil
         .          .    100:	}
         .       10ms    101:	ownPrice_string, _ := pt.priceTableMap.LoadOrStore("ownprice", pt.initprice)
         .          .    102:	ownPrice := ownPrice_string.(int64)
         .          .    103:	downstreamPrice, _ := pt.RetrieveDSPrice(ctx, methodName)
         .          .    104:	totalPrice := ownPrice + downstreamPrice
         .          .    105:	// totalPrice, _ := pt.RetrieveTotalPrice(ctx, methodName)
         .          .    106:
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).SplitTokens in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/tokenAndPrice.go
         0       10ms (flat, cum)  0.94% of Total
         .          .     11:func (pt *PriceTable) SplitTokens(ctx context.Context, tokenleft int64, methodName string) ([]string, error) {
         .          .     12:	downstreamNames, _ := pt.callMap[methodName]
         .          .     13:	size := len(downstreamNames)
         .          .     14:	if size == 0 {
         .          .     15:		return nil, nil
         .          .     16:	}
         .          .     17:
         .          .     18:	downstreamTokens := []string{}
         .          .     19:	downstreamPriceSum, _ := pt.RetrieveDSPrice(ctx, methodName)
         .          .     20:	tokenleftPerDownstream := (tokenleft - downstreamPriceSum) / int64(size)
         .          .     21:
         .          .     22:	pt.logger(ctx, "[Split tokens]: downstream total price is %d, from %d downstream services for %s, extra token left for each ds is %d\n",
         .       10ms     23:		downstreamPriceSum, size, pt.nodeName, tokenleftPerDownstream)
         .          .     24:
         .          .     25:	for _, downstreamName := range downstreamNames {
         .          .     26:		// concatenate the method name with node name to distinguish different downstream services calls.
         .          .     27:		downstreamPriceString, _ := pt.priceTableMap.LoadOrStore(methodName+"-"+downstreamName, pt.initprice)
         .          .     28:		downstreamPrice := downstreamPriceString.(int64)
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).UnaryInterceptor in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/charon.go
         0      240ms (flat, cum) 22.64% of Total
         .          .    319:func (pt *PriceTable) UnaryInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
         .          .    320:	// This is the server side interceptor, it should check tokens, update price, do overload handling and attach price to response
         .          .    321:	startTime := time.Now()
         .          .    322:
         .       20ms    323:	md, ok := metadata.FromIncomingContext(ctx)
         .          .    324:	if !ok {
         .          .    325:		return nil, errMissingMetadata
         .          .    326:	}
         .          .    327:
         .          .    328:	// print all the k-v pairs in the metadata md
         .          .    329:	// for k, v := range md {
         .          .    330:	// 	pt.logger(ctx, "[Received Req]:	The metadata for request is %s: %s\n", k, v)
         .          .    331:	// }
         .          .    332:	var metadataLog string
         .          .    333:	for k, v := range md {
         .       50ms    334:		metadataLog += fmt.Sprintf("%s: %s, ", k, v)
         .          .    335:	}
         .          .    336:	if metadataLog != "" {
         .          .    337:		pt.logger(ctx, "[Received Req]: The metadata for request is %s\n", metadataLog)
         .          .    338:	}
         .          .    339:
         .          .    340:	// Jiali: overload handler, do AQM, deduct the tokens on the request, update price info
         .          .    341:	var tok int64
         .          .    342:	var err error
         .          .    343:
         .       10ms    344:	if val, ok := md["tokens-"+pt.nodeName]; ok {
         .          .    345:		pt.logger(ctx, "[Received Req]:	tokens for %s are %s\n", pt.nodeName, val)
         .          .    346:		// raise error if the val length is not 1
         .          .    347:		if len(val) > 1 {
         .          .    348:			return nil, status.Errorf(codes.InvalidArgument, "duplicated tokens")
         .          .    349:		} else if len(val) == 0 {
         .          .    350:			return nil, errMissingMetadata
         .          .    351:		}
         .          .    352:		tok, err = strconv.ParseInt(val[0], 10, 64)
         .          .    353:	} else {
         .          .    354:		pt.logger(ctx, "[Received Req]:	tokens are %s\n", md["tokens"])
         .          .    355:		// raise error if the tokens length is not 1
         .          .    356:		if len(md["tokens"]) > 1 {
         .          .    357:			return nil, status.Errorf(codes.InvalidArgument, "duplicated tokens")
         .          .    358:		} else if len(md["tokens"]) == 0 {
         .          .    359:			return nil, errMissingMetadata
         .          .    360:		}
         .          .    361:		tok, err = strconv.ParseInt(md["tokens"][0], 10, 64)
         .          .    362:	}
         .          .    363:
         .          .    364:	// overload handler:
         .          .    365:	methodName := md["method"][0]
         .       10ms    366:	tokenleft, err := pt.LoadShedding(ctx, tok, methodName)
         .          .    367:	if err == InsufficientTokens && pt.loadShedding {
         .          .    368:		price_string, _ := pt.RetrieveTotalPrice(ctx, methodName)
         .          .    369:		header := metadata.Pairs("price", price_string, "name", pt.nodeName)
         .          .    370:		pt.logger(ctx, "[Sending Error Resp]:	Total price is %s\n", price_string)
         .          .    371:		grpc.SendHeader(ctx, header)
         .          .    372:
         .          .    373:		totalLatency := time.Since(startTime)
         .          .    374:		pt.logger(ctx, "[Server-side Timer] Processing Duration is: %.2d milliseconds\n", totalLatency.Milliseconds())
         .          .    375:
         .          .    376:		// if pt.pinpointLatency {
         .          .    377:		// 	if totalLatency > pt.observedDelay {
         .          .    378:		// 		pt.observedDelay = totalLatency // update the observed delay
         .          .    379:		// 	}
         .          .    380:		// }
         .          .    381:		// return nil, status.Errorf(codes.ResourceExhausted, "req dropped, try again later")
         .          .    382:		return nil, status.Errorf(codes.ResourceExhausted, "%s req dropped by %s. %d token for %s price. Try again later.", methodName, pt.nodeName, tok, price_string)
         .          .    383:	}
         .          .    384:	if err != nil && err != InsufficientTokens {
         .          .    385:		// The limiter failed. This error should be logged and examined.
         .          .    386:		log.Println(err)
         .          .    387:		return nil, status.Error(codes.Internal, "internal error")
         .          .    388:	}
         .          .    389:
         .          .    390:	tok_string := strconv.FormatInt(tokenleft, 10)
         .          .    391:	pt.logger(ctx, "[Preparing Sub Req]:	Token left is %s\n", tok_string)
         .          .    392:
         .          .    393:	// [critical] Jiali: Being outgoing seems to be critical for us.
         .          .    394:	// Jiali: we need to attach the token info to the context, so that the downstream can retrieve it.
         .          .    395:	// ctx = metadata.AppendToOutgoingContext(ctx, "tokens", tok_string)
         .          .    396:	// Jiali: we actually need multiple kv pairs for the token information, because one context is sent to multiple downstreams.
         .       10ms    397:	downstreamTokens, _ := pt.SplitTokens(ctx, tokenleft, methodName)
         .          .    398:
         .          .    399:	ctx = metadata.AppendToOutgoingContext(ctx, downstreamTokens...)
         .          .    400:
         .          .    401:	// queuingDelay := time.Since(startTime)
         .          .    402:	// pt.logger(ctx, "[Server-side Timer] Queuing delay is: %.2d milliseconds\n", queuingDelay.Milliseconds())
         .          .    403:
         .          .    404:	// if pt.pinpointQueuing {
         .          .    405:	// 	// increment the counter and add the queuing delay to the observed delay
         .          .    406:	// 	pt.Increment()
         .          .    407:	// 	pt.observedDelay += queuingDelay
         .          .    408:	// }
         .          .    409:
         .          .    410:	totalLatency := time.Since(startTime)
         .          .    411:	// log the total latency in unit of millisecond, decimal precision 2
         .          .    412:	pt.logger(ctx, "[Server-side Interceptor] Overhead is: %.2f milliseconds\n", float64(totalLatency.Microseconds())/1000)
         .          .    413:
         .      120ms    414:	m, err := handler(ctx, req)
         .          .    415:
         .          .    416:	// Attach the price info to response before sending
         .          .    417:	// right now let's just propagate the corresponding price of the RPC method rather than a whole pricetable.
         .          .    418:	// totalPrice_string, _ := PriceTableInstance.ptmap.Load("totalprice")
         .          .    419:
         .          .    420:	// if not pt.lazyResponse
         .          .    421:	if !pt.lazyResponse {
         .          .    422:		price_string, _ := pt.RetrieveTotalPrice(ctx, methodName)
         .       10ms    423:		header := metadata.Pairs("price", price_string, "name", pt.nodeName)
         .          .    424:		pt.logger(ctx, "[Preparing Resp]:	Total price of %s is %s\n", methodName, price_string)
         .       10ms    425:		grpc.SendHeader(ctx, header)
         .          .    426:	} else {
         .          .    427:		pt.logger(ctx, "[Preparing Resp]:	Lazy response is enabled, no price attached to response.\n")
         .          .    428:	}
         .          .    429:
         .          .    430:	if pt.pinpointLatency {
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).UnaryInterceptorClient in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/charon.go
         0      140ms (flat, cum) 13.21% of Total
         .          .    134:func (pt *PriceTable) UnaryInterceptorClient(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {
         .          .    135:	// Jiali: the following line print the method name of the req/response, will be used to update the
         .          .    136:	md, _ := metadata.FromOutgoingContext(ctx)
         .          .    137:	methodName := md["method"][0]
         .          .    138:	pt.logger(ctx, "[Before Sub Req]:	Node %s calling %s\n", pt.nodeName, methodName)
         .          .    139:	// Jiali: before sending. check the price, calculate the #tokens to add to request, update the total tokens
         .          .    140:	// overwrite rather than append to the header with the node name of this client
         .          .    141:	ctx = metadata.AppendToOutgoingContext(ctx, "name", pt.nodeName)
         .          .    142:	var header metadata.MD // variable to store header and trailer
         .      130ms    143:	err := invoker(ctx, method, req, reply, cc, grpc.Header(&header))
         .          .    144:
         .          .    145:	// run the following code asynchorously, without blocking the main thread.
         .          .    146:	// go func() {
         .          .    147:	// Jiali: after replied. update and store the price info for future
         .          .    148:	if len(header["price"]) > 0 {
         .          .    149:		priceDownstream, _ := strconv.ParseInt(header["price"][0], 10, 64)
         .       10ms    150:		pt.UpdateDownstreamPrice(ctx, methodName, header["name"][0], priceDownstream)
         .          .    151:		pt.logger(ctx, "[After Resp]:	The price table is from %s\n", header["name"])
         .          .    152:	} else {
         .          .    153:		pt.logger(ctx, "[After Resp]:	No price table received\n")
         .          .    154:	}
         .          .    155:	// }()
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).UpdateDownstreamPrice in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/tokenAndPrice.go
         0       10ms (flat, cum)  0.94% of Total
         .          .    200:func (pt *PriceTable) UpdateDownstreamPrice(ctx context.Context, method string, nodeName string, downstreamPrice int64) (int64, error) {
         .          .    201:
         .          .    202:	// Update the downstream price, but concatenate the method name with node name to distinguish different downstream services calls.
         .       10ms    203:	pt.priceTableMap.Store(method+"-"+nodeName, downstreamPrice)
         .          .    204:	pt.logger(ctx, "[Received Resp]:	Downstream price of %s updated to %d\n", method+"-"+nodeName, downstreamPrice)
         .          .    205:	pt.SaveDSPrice(ctx, method)
         .          .    206:	return downstreamPrice, nil
         .          .    207:}
ROUTINE ======================== github.com/tgiannoukos/charon.(*PriceTable).queuingCheck in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/overloadDetection.go
         0       20ms (flat, cum)  1.89% of Total
         .          .     37:func (pt *PriceTable) queuingCheck() {
         .          .     38:	// init a null histogram
         .          .     39:	var prevHist *metrics.Float64Histogram
         .          .     40:	for range time.Tick(pt.priceUpdateRate) {
         .          .     41:		// start a timer to measure the query latency
         .          .     42:		start := time.Now()
         .          .     43:		// get the current histogram
         .       10ms     44:		currHist := readHistogram()
         .          .     45:		/*
         .          .     46:			// calculate the differernce between the two histograms prevHist and currHist
         .          .     47:			diff := metrics.Float64Histogram{}
         .          .     48:			// if preHist is empty pointer, return currHist
         .          .     49:			if prevHist == nil {
         .          .     50:				diff = *currHist
         .          .     51:			} else {
         .          .     52:				diff = GetHistogramDifference(*prevHist, *currHist)
         .          .     53:			}
         .          .     54:			// maxLatency is the max of the histogram in milliseconds.
         .          .     55:			gapLatency := maximumBucket(&diff)
         .          .     56:		*/
         .          .     57:		if prevHist == nil {
         .          .     58:			// directly go to next iteration
         .          .     59:			prevHist = currHist
         .          .     60:			continue
         .          .     61:		}
         .          .     62:		gapLatency := maximumQueuingDelayms(prevHist, currHist)
         .          .     63:		// medianLatency := medianBucket(&diff)
         .          .     64:		// gapLatency := percentileBucket(&diff, 90)
         .          .     65:
         .       10ms     66:		ctx := metadata.NewIncomingContext(context.Background(), metadata.Pairs("request-id", "0"))
         .          .     67:
         .          .     68:		// ToDo: move the print of the histogram to a file
         .          .     69:		/*
         .          .     70:			cumulativeLat := medianBucket(currHist)
         .          .     71:			// printHistogram(currHist)
ROUTINE ======================== github.com/tgiannoukos/charon.readHistogram in /go/pkg/mod/github.com/tgiannoukos/charon@v0.0.0-20230929184655-44ffc2071f6d/queuingDelay.go
         0       10ms (flat, cum)  0.94% of Total
         .          .    133:func readHistogram() *metrics.Float64Histogram {
         .          .    134:	// Create a sample for metric /sched/latencies:seconds and /sync/mutex/wait/total:seconds
         .          .    135:	const queueingDelay = "/sched/latencies:seconds"
         .          .    136:	measureMutexWait := false
         .          .    137:
         .          .    138:	// Create a sample for the metric.
         .          .    139:	sample := make([]metrics.Sample, 1)
         .          .    140:	sample[0].Name = queueingDelay
         .          .    141:	if measureMutexWait {
         .          .    142:		const mutexWait = "/sync/mutex/wait/total:seconds"
         .          .    143:		sample[1].Name = mutexWait
         .          .    144:	}
         .          .    145:
         .          .    146:	// Sample the metric.
         .       10ms    147:	metrics.Read(sample)
         .          .    148:
         .          .    149:	// Check if the metric is actually supported.
         .          .    150:	// If it's not, the resulting value will always have
         .          .    151:	// kind KindBad.
         .          .    152:	if sample[0].Value.Kind() == metrics.KindBad {
